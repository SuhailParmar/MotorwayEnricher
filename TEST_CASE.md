# __Example of the suite working End to End__

The aim of this project was to gain context of incidents on the motorway using twitter. The first stage of this is the tweets retrieval segment. After research I found that the highway agencies have cameras all along the motorway and incidents are viewable through a URL. These cameras are linked to a twitter bot ```@Traffic_M**```.

These bot accounts tweet abouts incidents immediately, incidents include congestion, accidents, roadworks, lane blockings etc. These tweets are treated as reliable, as they are from the Highways agency, who manage the motorway.

In order to obtain the tweets I created a scraping tool, written in python, to connect to the TwitterAPI and get the latest tweets of the chosen motorway. I have decided to design this project using the microservices design pattern. All of these components will be containerised and deployed to an orchestration platform.

## __TwitterScraper__

The scraper can be run periodically and is designed to allow multiple instances of the scraper to run and connect to a different twitter account, and poll their latest n tweets.

I turned the scraper on around 7:00 on the 28th of Jan. It picked up this tweet.

### Original Tweet
```json
{
    "screen_name": "Traffic_M25",
    "created_at": "Mon Jan 28 05:00:45 +0000 2019",
    "id": 103,
    "payload": "#M25 clockwise between J8 (Reigate / Redhill) and J9 (Leatherhead / Epsom) - General Obstruction - Full details at https://www.MotorwayCameras.co.uk/Traffic#M25  (Updated every 5 minutes)"
}
```

This tweet is the raw responce from twitter, which needs to first be mined, for my applications to understand the tweet. This raw tweet is placed on a message queue in order for the next service to mine it.

## __Message Passing__

As i wanted to develop a system in an agile methodology - utilizing microservices, the first question was how to pass messages from one component to another. After research an industry standard was AMQP, an agnostic message protocol designed for middleware.

```css
http://localhost:15672/#/queues
```

## __Tweet (Miner/Converter)__

The aim of this component is to create a canonical json event, which can be sent to the API and saved in its database.

#### Key steps

1. It first notes down important information, such as the unique ID, time it occured and the motorway it's reporting.

2. The Converter proceeds to mine the payload, regex matching specific areas such as the Junction J**, splitting the tweet into 3 sections, the location, reason and the further information.

3. It also extracts the cities involved through bracket expansion and extracts the direction of travel.

### Json String
```json
"{\"metadata\": \"Event Generated by Tweet Miner at 2019-01-29T11:10:16.004379\", \"motorway\": 25, \"event_id\": 103, \"junction\": [8, 9], \"direction\": \"c\", \"closest_cities\": [\"Reigate / Redhill\", \"Leatherhead / Epsom\"], \"reason\": \"general obstruction\", \"extra_information\": \"\", \"time_day_worded\": \"Mon\", \"time_timestamp\": \"2019-01-28T05:00:45\", \"time_day_numerical\": 28, \"time_year\": 2019, \"time_hour\": 5, \"time_minutes\": 0, \"time_seconds\": 45}"
```
### Pretty Printed Json
```json
{
   "metadata":"Event Generated by Tweet Miner at 2019-01-28T11:44:48.106533",
   "motorway":25,
   "event_id":104,
   "junction":[
      8,
      9
   ],
   "direction":"c",
   "closest_cities":[
      "Reigate / Redhill",
      "Leatherhead / Epsom"
   ],
   "reason":"general obstruction",
   "extra_information":"",
   "time_day_worded":"Mon",
   "time_timestamp":"2019-01-28T05:00:45",
   "time_day_numerical":28,
   "time_year":2019,
   "time_hour":5,
   "time_minutes":0,
   "time_seconds":45
}
```

4. Once the mined tweet has been created it is then posted to the MotorwayAPI and the Enriched rabbit queue.


```
POST successful...
```

## __MotorwayAPI__

The API is key for holding and retrieving motorway event information. This part is written using Django, a python framework for building restful services. The API is protected using OAUTH2 and can be accessed through a few endpoints.

```
GET /api/events/all
GET /api/events/100..
DEL /api/events/100..
POST /api/events/
GET /events?<params>
```

The backend database is using postgres.

## __Tweet Enricher__

Once the enricher has recieved the converted message from the queue the aim is to try and gain some context, as to the cause of the incident. Most are due to accidents or just to a high volume of vehicles.

For debugging purposes:
```python
The enricher first polls Tier1 tweeters, ones which are known to tweet semi-frequently on motorway incidents such as:

T1_HANDLES = [
    "CtrafficUK",
    "TomBurnett88",
    "HighwaysEngland",
    "HighwaysWMIDS",
    "HighwaysNWEST",
    "HighwaysSEAST",
    "HighwaysEAST",
]
```

#### Steps of the Enricher

1. Inspect the tweet to obtain key information e.g. the timestamp, motorway / junction in question.

2. Construct a list of english keywords based on the information provided, this is to be able to build a set of query parameters to search twiter on.

```python
['M25', 'J8', 'J9']['clockwise', 'general', 'obstruction', 'reigate', 'redhill', 'leatherhead', 'epsom']
```

3. The enricher then creates a query param e.g. ``` M6 & J25 ``` to send to the twitter api. It also creates a set of time boundaries, in order to narrow the search. e.g. if the original event came in at ```2nd Jan 10:00``` then the enricher would search between ```2nd Jan 8:00 and 2nd Jan 14:00```.

4. It utilizes twitters paging algorithm to constantly search 'new tweets'. If a tweet is in range it then decides if tit's relevant. It does so by ignoring retweets, regex matching words and ensuring the new tweet is regarding the same incident, e.g. if it's about the same direction.

5. Before key analysis the important step is stripping away words 'stop words' alongside words with little meaning, such as 'due, to , earlier, this is to prevent the TF*IDF matrix weighting words which arent useful to us.

6. I wanted to create my own TF*IDF matrix, but the results of the sklearn one was much better, alongside being a solid and fully tested package it provides added extra functionality such as the cosine-similarity function to judge the similarity between documents and weight the words accordingly.

7. Once we have obtained a normalised TF*IDF matrix of the tweets (documents) gathered the aim is then to cluster terms in order to find the hidden structure between specific weighted words. For example if, as in this case a lorry tyre caused the incident lorry and tyre may be weighted differently but may be clustered together due to the words appearing near each other.

8. To cluster I have used K-Means to identify the n words closest to the cluster centroid, as the centroid contains the highest value words.

9. Once i have achieved this information, compile a json payload with the information in it and post it to the original tweet ID.
```json
{
    "extra_information": [
        "lorry",
        "tyre",
        "j7",
        "congestion",
        "a217"
    ]
}

```